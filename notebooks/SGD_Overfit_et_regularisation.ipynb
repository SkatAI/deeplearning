{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SGD et overfit\n",
        "\n",
        "Dans ce notebook nous allons entrainer un modele de gradient stochastique de scikit-learn et montrer comment detecter l'overfit\n",
        "\n",
        "Dans un second temps nous appliquerons de la régularisation L2 pour compenser cet overfit.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ow9NbnKRHLTm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Zgh6vkkHFmI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils import shuffle\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Création du dataset\n",
        "\n",
        "Plutot que d'utiliser un vrai jeu de données, nous allons le créer avec [make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification) de scikit-learn."
      ],
      "metadata": {
        "id": "gZFxElEeH6VP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_classification(n_samples=200, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=42)"
      ],
      "metadata": {
        "id": "SgwySYRTH5QQ"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='k', s=50)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Scatter Plot')\n",
        "plt.colorbar(label='Catégorie')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nI3D1_BaIUd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and test sets with a smaller training set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)\n"
      ],
      "metadata": {
        "id": "Bptgu_sKISvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Le modèle\n",
        "\n",
        "Soit un [Stochastic Gradient Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) en veillant à supprimer la regularisation : `penalty=None`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CtXbPnncJFUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = SGDClassifier(penalty=None,  random_state=42, loss='log_loss')\n",
        "clf"
      ],
      "metadata": {
        "id": "d4am0OnWJFB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La fonction suivante permet de visualier l'evolution de la fonction de cout au fil des iterations\n",
        "\n"
      ],
      "metadata": {
        "id": "AIUCdUg2JqoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss():\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(range(n_epochs), test_losses, label='Test Loss')\n",
        "    plt.plot(range(n_epochs), train_losses, label='Training Loss')\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Log Loss')\n",
        "    plt.title('Log Loss par Iterations')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "sh-rYkajJBii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plutot que d'entraîner le modele avec la fonction `fit()`, on utilise la fonction `partial_fit()` qui permet d'entrainer le modèle pas à pas pour observer son évolution à chaque itération."
      ],
      "metadata": {
        "id": "XVHB9zMbJ7p5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X_train, y_train, X_test, y_test):\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
        "\n",
        "        clf.partial_fit(X_train, y_train, classes=np.unique(y))\n",
        "\n",
        "        y_train_pred_proba = clf.predict_proba(X_train)\n",
        "        y_test_pred_proba = clf.predict_proba(X_test)\n",
        "\n",
        "        train_loss = log_loss(y_train, y_train_pred_proba)\n",
        "        test_loss = log_loss(y_test, y_test_pred_proba)\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(test_loss)\n",
        "    return train_losses, test_losses"
      ],
      "metadata": {
        "id": "atf67_fYJ54m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval():\n",
        "    y_train_pred = clf.predict(X_train)\n",
        "    y_test_pred = clf.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "    print(f'Training accuracy: {train_accuracy:.2f}')\n",
        "    print(f'Test accuracy: {test_accuracy:.2f}')"
      ],
      "metadata": {
        "id": "ezMIoJsCMEzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# entrainement sans regularisation"
      ],
      "metadata": {
        "id": "6C92NllcKmkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 1000\n",
        "clf = SGDClassifier(penalty=None,  random_state=42, loss='log_loss', learning_rate = 'constant', eta0 = 0.01)\n",
        "train_losses, test_losses = train(X_train, y_train, X_test, y_test)\n",
        "eval()\n",
        "plot_loss()"
      ],
      "metadata": {
        "id": "-NElM81cKj8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A quoi voyez-vous que le modele overfit ?"
      ],
      "metadata": {
        "id": "frORYrvnV7L1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A vous\n",
        "\n",
        "Ajoutez une regularisation L2 au model avec un alpha = 0.01 et observez la fonction de cout"
      ],
      "metadata": {
        "id": "AjwqIlx_MiPN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Que peut on faire pour reduire encore l'overfit ?\n",
        "\n",
        "- augmenter encore la regularisation\n",
        "- utiliser plus  des données pour l'entrainement\n"
      ],
      "metadata": {
        "id": "WYaCt-XuQ8Dp"
      }
    }
  ]
}