{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]",
    "(https://colab.research.google.com/github/SkatAI/deeplearning/blob/master/notebooks/moliere_lstm_generation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1076412",
   "metadata": {},
   "source": [
    "# Génération de texte avec un LSTM : écrire comme Molière\n",
    "\n",
    "**Objectif** : Entraîner un réseau LSTM character-level à générer du texte dans le style de Molière.\n",
    "\n",
    "**Ce que vous allez apprendre** :\n",
    "- Comment transformer du texte en séquences pour un réseau de neurones\n",
    "- Comment fonctionne un LSTM pour la modélisation de séquences\n",
    "- La différence entre prédiction de séries temporelles et génération de texte (spoiler : c'est le même principe !)\n",
    "- Le rôle de la **temperature** dans la génération\n",
    "- Le lien avec les LLMs modernes (GPT, Claude, Gemini...)\n",
    "\n",
    "**Prérequis** : RNN, LSTM, GRU sur séries temporelles.\n",
    "\n",
    "**Environnement** : Google Colab (free tier, CPU ou GPU T4)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2bae98",
   "metadata": {},
   "source": [
    "## 1. L'intuition : des séries temporelles au texte\n",
    "\n",
    "Vous avez déjà utilisé des LSTM pour prédire des séries temporelles. Le principe était :\n",
    "\n",
    "> Étant donné une séquence de valeurs passées $[x_1, x_2, ..., x_t]$, prédire la prochaine valeur $x_{t+1}$\n",
    "\n",
    "La génération de texte, c'est **exactement la même chose**, sauf qu'au lieu de prédire un nombre, on prédit le **prochain caractère** (ou mot) :\n",
    "\n",
    "> Étant donné `\"Quand l'Amour à vos yeux offre un choi\"` → prédire `\"x\"`\n",
    "\n",
    "C'est le principe fondamental derrière **tous** les modèles de langage actuels, y compris GPT-4 ou Claude. La différence ? L'échelle (milliards de paramètres, architecture Transformer), mais le concept de base est le même : **next token prediction**.\n",
    "\n",
    "Dans ce TD, on va travailler au niveau **caractère** (char-level). Chaque \"token\" est une lettre, un espace, ou un signe de ponctuation. C'est plus simple qu'un tokenizer par mots ou sous-mots, et ça permet d'obtenir des résultats intéressants même avec un petit modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39778624",
   "metadata": {},
   "source": [
    "## 2. Chargement des données\n",
    "\n",
    "On utilise un corpus de pièces de Molière, disponible en CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afde99f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Chargement du dataset\n",
    "url = \"https://skatai.com/data/df_moliere.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(f\"Nombre de répliques : {len(df)}\")\n",
    "print(f\"Colonnes : {list(df.columns)}\")\n",
    "print(f\"Pièces : {df['play_name'].nunique()}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a372b69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On concatène toutes les répliques en un seul texte\n",
    "# On ajoute un retour à la ligne entre chaque réplique pour garder la structure\n",
    "text = \"\\n\".join(df[\"cue\"].dropna().astype(str).values)\n",
    "\n",
    "print(f\"Longueur du texte : {len(text):,} caractères\")\n",
    "print(f\"\\nExtrait :\")\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433bdf81",
   "metadata": {},
   "source": [
    "## 3. Tokenization character-level\n",
    "\n",
    "En char-level, chaque caractère unique du corpus devient un \"token\". Notre vocabulaire est l'ensemble des caractères distincts.\n",
    "\n",
    "Comparez avec les séries temporelles : là-bas vos données étaient déjà numériques. Ici, on doit d'abord **encoder** les caractères en nombres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17db3064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Construction du vocabulaire avec filtrage des caractères rares\n",
    "# -------------------------------------------------------------\n",
    "# MIN_CHAR_FREQ = 1  -> aucun filtrage (vocabulaire complet)\n",
    "# MIN_CHAR_FREQ = 3+ -> on remplace les caractères très rares par <UNK>\n",
    "MIN_CHAR_FREQ = 3\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "char_counts = Counter(text)\n",
    "kept_char_list = sorted([ch for ch, n in char_counts.items() if n >= MIN_CHAR_FREQ])\n",
    "removed_char_list = sorted([ch for ch, n in char_counts.items() if n < MIN_CHAR_FREQ])\n",
    "\n",
    "chars = kept_char_list.copy()\n",
    "\n",
    "# On garantit la présence d'un espace pour le padding de génération\n",
    "if ' ' not in chars:\n",
    "    chars.append(' ')\n",
    "    chars = sorted(chars)\n",
    "\n",
    "# Ajout du token inconnu en fin de vocabulaire\n",
    "chars.append(UNK_TOKEN)\n",
    "\n",
    "vocab_size = len(chars)\n",
    "kept_chars = len(kept_char_list)\n",
    "removed_chars = len(removed_char_list)\n",
    "\n",
    "print(f\"MIN_CHAR_FREQ : {MIN_CHAR_FREQ}\")\n",
    "print(f\"Taille du vocabulaire : {vocab_size} tokens\")\n",
    "print(f\"Caractères conservés ({kept_chars}) : {kept_char_list}\")\n",
    "print(f\"Caractères rares remplacés par {UNK_TOKEN} ({removed_chars}) : {removed_char_list}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dff1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaires de correspondance caractère <-> index\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "unk_idx = char_to_idx[UNK_TOKEN]\n",
    "\n",
    "# Encoder tout le texte en séquence d'entiers\n",
    "text_encoded = np.array([char_to_idx.get(ch, unk_idx) for ch in text], dtype=np.int32)\n",
    "unk_count = int(np.sum(text_encoded == unk_idx))\n",
    "\n",
    "print(f\"Texte original  : {text[:50]}\")\n",
    "print(f\"Texte encodé    : {text_encoded[:50]}\")\n",
    "print(f\"\\nExemple : '{text[0]}' -> {text_encoded[0]} -> '{idx_to_char[text_encoded[0]]}'\")\n",
    "print(f\"Nombre de tokens {UNK_TOKEN} dans le corpus : {unk_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f50228d",
   "metadata": {},
   "source": [
    "### Comparaison avec les séries temporelles\n",
    "\n",
    "| | Séries temporelles | Texte (char-level) |\n",
    "|---|---|---|\n",
    "| **Token** | Valeur numérique (prix, température...) | Un caractère (lettre, espace, ponctuation) |\n",
    "| **Vocabulaire** | Continu (infini) | Fini (~70-100 caractères) |\n",
    "| **Encoding** | Pas nécessaire | char → index entier → embedding |\n",
    "| **Prédiction** | Régression (valeur continue) | Classification (quel caractère parmi N ?) |\n",
    "| **Loss** | MSE | Cross-entropy |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0c3195",
   "metadata": {},
   "source": [
    "## 4. Création des séquences d'entraînement\n",
    "\n",
    "Comme pour les séries temporelles, on crée des fenêtres glissantes. Pour chaque fenêtre de `SEQ_LENGTH` caractères, la cible est le caractère suivant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline `tf.data` pas à pas (avec échantillons)\n",
    "\n",
    "On décompose volontairement la construction du dataset pour inspecter le contenu à chaque étape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# ============================================================\n",
    "# >>> À EXPÉRIMENTER : changez SEQ_LENGTH et observez l'effet\n",
    "# Valeurs suggérées : 40, 60, 100, 150\n",
    "# ============================================================\n",
    "SEQ_LENGTH = 40\n",
    "BATCH_SIZE = 64\n",
    "WINDOW_SHIFT = 5\n",
    "\n",
    "# Étape 1: un dataset de scalaires (1 index de caractère par élément)\n",
    "tokens_ds = tf.data.Dataset.from_tensor_slices(text_encoded)\n",
    "\n",
    "print(\"Element spec tokens_ds:\", tokens_ds.element_spec)\n",
    "for token in tokens_ds.take(8):\n",
    "    print(int(token.numpy()), end=' ')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 2: fenêtres glissantes de longueur SEQ_LENGTH + 1\n",
    "windows_ds = tokens_ds.window(SEQ_LENGTH + 1, shift=WINDOW_SHIFT, drop_remainder=True)\n",
    "\n",
    "# Un élément de windows_ds est lui-même un Dataset (fenêtre)\n",
    "first_window_ds = next(iter(windows_ds.take(1)))\n",
    "first_window_tokens = list(first_window_ds.as_numpy_iterator())\n",
    "\n",
    "print(\"Type d'un élément de windows_ds:\", type(first_window_ds))\n",
    "print(\"Longueur première fenêtre:\", len(first_window_tokens))\n",
    "print(\"Premiers tokens fenêtre:\", first_window_tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 3: conversion de chaque fenêtre en tenseur [SEQ_LENGTH + 1]\n",
    "sequences = windows_ds.flat_map(lambda w: w.batch(SEQ_LENGTH + 1))\n",
    "\n",
    "print(\"Element spec sequences:\", sequences.element_spec)\n",
    "example_sequence = next(iter(sequences.take(1))).numpy()\n",
    "print(\"Shape d'une séquence:\", example_sequence.shape)\n",
    "print(\"Premiers tokens séquence:\", example_sequence[:10])\n",
    "\n",
    "\n",
    "def split_input_target(sequence):\n",
    "    \"\"\"Sépare chaque séquence en input (tous sauf le dernier) et target (tous sauf le premier).\"\"\"\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "# Étape 4: mapping vers (input, target)\n",
    "pairs_ds = sequences.map(split_input_target, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Element spec pairs_ds:\", pairs_ds.element_spec)\n",
    "inp, tgt = next(iter(pairs_ds.take(1)))\n",
    "print(\"Shape input:\", inp.shape, \"| Shape target:\", tgt.shape)\n",
    "print(\"Input (début):\", inp.numpy()[:10])\n",
    "print(\"Target(début):\", tgt.numpy()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 5: pipeline final pour l'entraînement\n",
    "num_tokens = len(text_encoded)\n",
    "num_sequences = max(0, (num_tokens - (SEQ_LENGTH + 1)) // WINDOW_SHIFT + 1)\n",
    "steps_per_epoch = num_sequences // BATCH_SIZE  # drop_remainder=True\n",
    "\n",
    "print(f\"Nombre total de tokens       : {num_tokens}\")\n",
    "print(f\"Nombre total de séquences    : {num_sequences}\")\n",
    "print(f\"Batch size                   : {BATCH_SIZE}\")\n",
    "print(f\"steps_per_epoch (estimé)     : {steps_per_epoch}\")\n",
    "\n",
    "if steps_per_epoch == 0:\n",
    "    raise ValueError(\"steps_per_epoch=0: augmentez les données ou réduisez BATCH_SIZE/SEQ_LENGTH.\")\n",
    "\n",
    "dataset = (\n",
    "    pairs_ds\n",
    "    .shuffle(10000)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Vérification batch final\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(f\"Shape input  : {input_example.shape}  (batch, seq_length)\")\n",
    "    print(f\"Shape target : {target_example.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84718805",
   "metadata": {},
   "source": [
    "**Remarquez** : le target est simplement l'input décalé d'une position. Pour chaque position $t$, le modèle doit prédire le caractère en position $t+1$. C'est exactement le même principe que le \"sliding window\" des séries temporelles.\n",
    "\n",
    "> **Question pour vous** : que se passe-t-il si on augmente `SEQ_LENGTH` ? Quel est le compromis ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069ae5f0",
   "metadata": {},
   "source": [
    "## 5. Construction du modèle LSTM\n",
    "\n",
    "L'architecture est simple :\n",
    "1. **[Embedding](https://keras.io/api/layers/core_layers/embedding/)** : transforme chaque index de caractère en un vecteur dense (comme un word2vec, mais pour des caractères)\n",
    "2. **LSTM** : apprend les dépendances séquentielles\n",
    "3. **Dense + softmax** : prédit une distribution de probabilité sur les `vocab_size` caractères possibles\n",
    "\n",
    "C'est un problème de **classification** à chaque pas de temps : parmi les ~80 caractères possibles, lequel vient ensuite ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03fb439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Mixed precision pour accélérer l'entraînement GPU (ex: Colab T4)\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "# ============================================================\n",
    "# >>> À EXPÉRIMENTER : modifiez ces hyperparamètres\n",
    "# EMBEDDING_DIM : 64, 128, 256\n",
    "# LSTM_UNITS : 128, 256, 512\n",
    "# Ajoutez un 2e layer LSTM (return_sequences=True sur le premier)\n",
    "# ============================================================\n",
    "EMBEDDING_DIM = 64\n",
    "LSTM_UNITS = 128\n",
    "\n",
    "model = Sequential([\n",
    "    # Embedding : chaque caractère (index) -> vecteur dense\n",
    "    Embedding(vocab_size, EMBEDDING_DIM, input_length=SEQ_LENGTH),\n",
    "\n",
    "    # LSTM : capture les dépendances dans la séquence\n",
    "    LSTM(LSTM_UNITS, return_sequences=True),\n",
    "\n",
    "    # Dropout pour la régularisation\n",
    "    Dropout(0.2),\n",
    "\n",
    "    # Couche de sortie : probabilité pour chaque caractère du vocabulaire\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d2fe6f",
   "metadata": {},
   "source": [
    "### Comprendre l'architecture\n",
    "\n",
    "- **Embedding(vocab_size, 64)** : Le modèle apprend une représentation vectorielle pour chaque caractère. Les caractères qui apparaissent dans des contextes similaires (ex: voyelles entre elles) auront des embeddings proches.\n",
    "- **LSTM(128)** : La couche LSTM lit la séquence caractère par caractère et maintient un état interne (cell state + hidden state). C'est cet état qui \"résume\" ce qui a été lu et permet de prédire la suite.\n",
    "- **Dense(vocab_size, softmax)** : La sortie est un vecteur de probabilités. Si le vocabulaire a 80 caractères, on obtient 80 probabilités qui somment à 1.\n",
    "\n",
    "**Pourquoi `sparse_categorical_crossentropy` et pas `categorical_crossentropy` ?**\n",
    "\n",
    "- Avec `sparse_categorical_crossentropy`, les labels restent des entiers (`y = 17`, `y = 42`, ...).\n",
    "- Avec `categorical_crossentropy`, il faudrait convertir chaque label en one-hot de taille `vocab_size`.\n",
    "- Ici nos targets sont des index entiers de caractères (shape `(batch, seq_length)`), donc `sparse_categorical_crossentropy` est la forme naturelle, plus simple et plus économe en mémoire.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38db9f64",
   "metadata": {},
   "source": [
    "## 6. Entraînement\n",
    "\n",
    "On entraîne sur quelques epochs. Sur free Colab (T4 GPU ou CPU), ça devrait prendre quelques minutes par epoch.\n",
    "\n",
    "**Pourquoi Colab affiche parfois `xx/Unknown` ?**\n",
    "\n",
    "Avec certains pipelines `tf.data` (notamment `window` + `flat_map`), TensorFlow ne connaît pas toujours la cardinalité exacte du dataset au moment de l'affichage de la barre de progression.\n",
    "\n",
    "Ici, on calcule explicitement `steps_per_epoch` dans la section 4, puis on le passe à `model.fit(...)` pour obtenir une progression `current/total`.\n",
    "\n",
    "**Astuce** : Activez le GPU dans Colab via `Runtime > Change runtime type > T4 GPU` pour accélérer l'entraînement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a192d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# >>> À EXPÉRIMENTER : nombre d'epochs\n",
    "# 5 epochs  : le modèle commence à apprendre la structure\n",
    "# 15 epochs : résultats intéressants\n",
    "# 30+ epochs : meilleure qualité (mais plus long)\n",
    "# ============================================================\n",
    "EPOCHS = 15\n",
    "\n",
    "history = model.fit(\n",
    "    dataset,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec26cad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(history.history['loss'])\n",
    "ax1.set_title('Loss par epoch')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(history.history['accuracy'])\n",
    "ax2.set_title('Accuracy par epoch')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nLoss finale : {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Accuracy finale : {history.history['accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarder, télécharger et recharger un modèle\n",
    "\n",
    "Après un entraînement court (ex: 3-5 epochs), vous pouvez télécharger ce modèle puis charger un modèle pré-entraîné plus long pour comparer la génération.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle entraîné dans ce notebook (entraînement court)\n",
    "quick_model = model\n",
    "pretrained_model = None\n",
    "inference_model = quick_model  # modèle utilisé par défaut pour la génération\n",
    "\n",
    "MODEL_SAVE_PATH = \"moliere_lstm_quick.keras\"\n",
    "quick_model.save(MODEL_SAVE_PATH)\n",
    "print(f\"Modèle sauvegardé: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "# Téléchargement (Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(MODEL_SAVE_PATH)\n",
    "except Exception:\n",
    "    print(\"Téléchargement auto indisponible hors Colab. Le fichier est sauvegardé localement.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionnel: charger un modèle pré-entraîné depuis un fichier local (upload Colab)\n",
    "# -------------------------------------------------------------------------------\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # choisissez un fichier .keras\n",
    "# uploaded_path = next(iter(uploaded.keys()))\n",
    "#\n",
    "# pretrained_model = tf.keras.models.load_model(uploaded_path)\n",
    "# inference_model = pretrained_model\n",
    "# print(f\"Modèle pré-entraîné chargé depuis upload: {uploaded_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionnel: charger un modèle pré-entraîné depuis une URL\n",
    "# -------------------------------------------------------\n",
    "# PRETRAINED_MODEL_URL = \"https://.../moliere_lstm_long.keras\"\n",
    "#\n",
    "# local_pretrained_path = tf.keras.utils.get_file(\n",
    "#     fname=\"moliere_lstm_pretrained.keras\",\n",
    "#     origin=PRETRAINED_MODEL_URL\n",
    "# )\n",
    "# pretrained_model = tf.keras.models.load_model(local_pretrained_path)\n",
    "# inference_model = pretrained_model\n",
    "# print(f\"Modèle pré-entraîné chargé depuis URL: {local_pretrained_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionnel: comparaison rapide entre modèle court et modèle pré-entraîné\n",
    "# Décommentez après avoir chargé pretrained_model\n",
    "# ---------------------------------------------------------------\n",
    "# seed_text_compare = \"Quand l'Amour\"\n",
    "# temperature_compare = 0.8\n",
    "#\n",
    "# if pretrained_model is None:\n",
    "#     print(\"Chargez d'abord un modèle pré-entraîné (upload ou URL).\")\n",
    "# else:\n",
    "#     print(\"--- Modèle entraîné rapidement (quick_model) ---\")\n",
    "#     print(generate_text(quick_model, seed_text_compare, num_generate=200, temperature=temperature_compare))\n",
    "#     print()\n",
    "#     print(\"--- Modèle pré-entraîné (long training) ---\")\n",
    "#     print(generate_text(pretrained_model, seed_text_compare, num_generate=200, temperature=temperature_compare))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2073315",
   "metadata": {},
   "source": [
    "## 7. Génération de texte et temperature\n",
    "\n",
    "Maintenant, on utilise le modèle pour **générer** du texte. Le processus est autorégressif :\n",
    "\n",
    "1. On donne une séquence de départ (seed)\n",
    "2. Le modèle prédit une distribution de probabilité sur le prochain caractère\n",
    "3. On **échantillonne** un caractère selon cette distribution\n",
    "4. On ajoute ce caractère à la séquence et on recommence\n",
    "\n",
    "### Le rôle de la temperature\n",
    "\n",
    "La **temperature** contrôle la \"créativité\" de la génération en modifiant la distribution de probabilité avant l'échantillonnage :\n",
    "\n",
    "$$p_i = \\frac{\\exp(\\log(p_i) / T)}{\\sum_j \\exp(\\log(p_j) / T)}$$\n",
    "\n",
    "- **T → 0** : le modèle choisit presque toujours le caractère le plus probable → texte répétitif, \"sûr\"\n",
    "- **T = 1** : distribution non modifiée → équilibre\n",
    "- **T > 1** : distribution plus uniforme → texte plus \"créatif\", plus de surprises, mais aussi plus d'erreurs\n",
    "\n",
    "C'est exactement le même paramètre que vous retrouvez dans les API de GPT, Claude, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07edf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, num_generate=300, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Génère du texte caractère par caractère.\n",
    "\n",
    "    Args:\n",
    "        model: le modèle LSTM entraîné\n",
    "        start_string: texte de départ (seed)\n",
    "        num_generate: nombre de caractères à générer\n",
    "        temperature: contrôle la créativité (0.2 = conservateur, 1.5 = créatif)\n",
    "    \"\"\"\n",
    "    unk_idx = char_to_idx[UNK_TOKEN]\n",
    "    pad_idx = char_to_idx.get(' ', unk_idx)\n",
    "\n",
    "    # Encoder le texte de départ (caractères inconnus -> <UNK>)\n",
    "    input_eval = [char_to_idx.get(ch, unk_idx) for ch in start_string]\n",
    "\n",
    "    # S'assurer que la séquence fait exactement SEQ_LENGTH\n",
    "    if len(input_eval) < SEQ_LENGTH:\n",
    "        # Padding à gauche avec des espaces\n",
    "        pad = [pad_idx] * (SEQ_LENGTH - len(input_eval))\n",
    "        input_eval = pad + input_eval\n",
    "    else:\n",
    "        input_eval = input_eval[-SEQ_LENGTH:]\n",
    "\n",
    "    generated = list(start_string)\n",
    "\n",
    "    for _ in range(num_generate):\n",
    "        input_array = np.array([input_eval])\n",
    "\n",
    "        # Prédiction : distribution de probabilité sur le vocabulaire\n",
    "        predictions = model.predict(input_array, verbose=0)[0, -1]\n",
    "\n",
    "        # Application de la temperature\n",
    "        predictions = np.log(predictions + 1e-8) / temperature\n",
    "        predictions = np.exp(predictions)\n",
    "        predictions = predictions / np.sum(predictions)\n",
    "\n",
    "        # Échantillonnage selon la distribution\n",
    "        predicted_id = np.random.choice(len(predictions), p=predictions)\n",
    "\n",
    "        # Ajouter le caractère généré et décaler la fenêtre\n",
    "        next_token = idx_to_char[predicted_id]\n",
    "        generated.append(next_token if next_token != UNK_TOKEN else '?')\n",
    "        input_eval = input_eval[1:] + [predicted_id]\n",
    "\n",
    "    return ''.join(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52543889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# >>> À EXPÉRIMENTER : changez la temperature et le seed\n",
    "# ============================================================\n",
    "\n",
    "seed_text = \"Quand l'Amour\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "for temp in [0.2, 0.5, 0.8, 1.0, 1.3]:\n",
    "    print(f\"\\n--- Temperature = {temp} ---\\n\")\n",
    "    result = generate_text(inference_model, seed_text, num_generate=200, temperature=temp)\n",
    "    print(result)\n",
    "    print()\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dad4c0",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "> **Exercice** : Comparez les textes générés aux différentes temperatures.\n",
    ">\n",
    "> 1. À temperature basse (0.2), que remarquez-vous ? Le texte est-il varié ?\n",
    "> 2. À temperature haute (1.3), la qualité diminue-t-elle ? Pourquoi ?\n",
    "> 3. Quelle temperature vous semble produire le meilleur compromis ?\n",
    ">\n",
    "> **Essayez aussi** avec différents textes de départ (seed). Le modèle génère-t-il différemment selon le contexte initial ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3581332c",
   "metadata": {},
   "source": [
    "## 8. À vous de jouer !\n",
    "\n",
    "### Expérience 1 : Impact de la longueur de séquence\n",
    "\n",
    "Revenez à la section 4 et changez `SEQ_LENGTH`. Essayez 40, 100, et 150. Réentraînez et comparez la qualité du texte généré.\n",
    "\n",
    "> Plus la séquence est longue, plus le modèle a de contexte, mais l'entraînement est plus lent et nécessite plus de mémoire.\n",
    "\n",
    "### Expérience 2 : Architecture du modèle\n",
    "\n",
    "Modifiez le modèle dans la section 5 :\n",
    "- Essayez un modèle plus petit (LSTM 128 units) vs plus grand (LSTM 512)\n",
    "- Ajoutez une deuxième couche LSTM :\n",
    "\n",
    "```python\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, EMBEDDING_DIM, input_length=SEQ_LENGTH),\n",
    "    LSTM(256, return_sequences=True),   # return_sequences=True pour empiler\n",
    "    LSTM(256, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "- Remplacez le LSTM par un GRU. Le résultat change-t-il ?\n",
    "\n",
    "### Expérience 3 : Sous-corpus\n",
    "\n",
    "Essayez d'entraîner sur une seule pièce. Le style est-il différent ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5332c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple : entraîner sur une seule pièce\n",
    "# Décommentez et adaptez :\n",
    "\n",
    "# piece = \"tartuffe\"  # ou \"misanthrope\", etc.\n",
    "# df_piece = df[df['play_name'] == piece]\n",
    "# text_piece = \"\\n\".join(df_piece[\"cue\"].dropna().astype(str).values)\n",
    "# print(f\"Pièce : {piece}, {len(text_piece):,} caractères\")\n",
    "# print(f\"Pièces disponibles : {sorted(df['play_name'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a42f95",
   "metadata": {},
   "source": [
    "## 9. Bonus : comparaison avec un modèle Hugging Face\n",
    "\n",
    "Pour mettre en perspective, comparons notre petit LSTM avec un modèle de langage pré-entraîné. On utilise un modèle GPT-2 (le plus petit, 124M paramètres) pour générer du texte à partir du même seed.\n",
    "\n",
    "Notre LSTM a quelques centaines de milliers de paramètres et a été entraîné sur quelques centaines de Ko de texte. GPT-2 \"small\" a 124 millions de paramètres et a été entraîné sur des milliards de mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fbe6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation si nécessaire (déjà installé sur Colab en général)\n",
    "# !pip install transformers -q\n",
    "\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "seed_text_hf = \"Quand l'Amour\"\n",
    "set_seed(42)\n",
    "\n",
    "# 1) GPT-2 (baseline)\n",
    "try:\n",
    "    generator_gpt2 = pipeline('text-generation', model='gpt2', device=-1)  # CPU\n",
    "\n",
    "    result_gpt2 = generator_gpt2(\n",
    "        seed_text_hf,\n",
    "        max_length=150,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.8,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    print(\"--- GPT-2 (124M params, entraîné majoritairement en anglais) ---\")\n",
    "    print(result_gpt2[0]['generated_text'])\n",
    "    print()\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors du chargement de GPT-2 : {e}\")\n",
    "    print()\n",
    "\n",
    "# 2) Modèle français: asi/gpt-fr-cased-small\n",
    "try:\n",
    "    generator_gpt_fr = pipeline('text-generation', model='asi/gpt-fr-cased-small', device=-1)  # CPU\n",
    "\n",
    "    result_gpt_fr = generator_gpt_fr(\n",
    "        seed_text_hf,\n",
    "        max_length=150,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.8,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    print(\"--- asi/gpt-fr-cased-small (modèle français) ---\")\n",
    "    print(result_gpt_fr[0]['generated_text'])\n",
    "    print()\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors du chargement de asi/gpt-fr-cased-small : {e}\")\n",
    "    print()\n",
    "\n",
    "# 3) Notre LSTM\n",
    "print(\"--- Notre LSTM (entraîné sur Molière uniquement) ---\")\n",
    "print(generate_text(inference_model, seed_text_hf, num_generate=150, temperature=0.8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694b9946",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "> **Question** : GPT-2 est entraîné principalement en anglais. Comment se comporte-t-il sur un prompt en français ? Pourquoi ?\n",
    ">\n",
    "> **Question** : Notre LSTM produit du texte \"à la Molière\" car il n'a vu que ça. C'est à la fois sa force (spécialisation) et sa limite (pas de connaissance générale). Comment les LLMs modernes gèrent-ils ce compromis ?\n",
    ">\n",
    "> **Pour aller plus loin** : essayez avec un modèle comme `\"asi/gpt-fr-cased-small\"` (GPT-2 entraîné sur du français) si disponible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2943c11",
   "metadata": {},
   "source": [
    "## 10. Récapitulatif et liens avec les LLMs\n",
    "\n",
    "### Ce qu'on a fait\n",
    "1. **Tokenization** : transformer du texte en séquence d'entiers (ici char-level)\n",
    "2. **Séquences** : créer des fenêtres glissantes input/target (comme en séries temporelles)\n",
    "3. **Modèle** : Embedding → LSTM → Dense (classification sur le vocabulaire)\n",
    "4. **Génération** : prédiction autoregressive + sampling avec temperature\n",
    "\n",
    "### Le pont vers les LLMs modernes\n",
    "\n",
    "| | Notre LSTM | GPT / Claude / Gemini |\n",
    "|---|---|---|\n",
    "| **Tokenization** | Character-level (~80 tokens) | Subword (BPE, ~50K-100K tokens) |\n",
    "| **Architecture** | LSTM | Transformer (attention) |\n",
    "| **Contexte** | 60-100 caractères | 8K - 1M+ tokens |\n",
    "| **Paramètres** | ~300K | 7B - 1000B+ |\n",
    "| **Entraînement** | Un corpus (~500 Ko) | Internet (~To de texte) |\n",
    "| **Principe** | Next token prediction | Next token prediction |\n",
    "\n",
    "Le principe fondamental est **identique** : prédire le prochain token. Ce qui change, c'est l'échelle, l'architecture (attention vs récurrence), et la quantité de données.\n",
    "\n",
    "### Pour aller plus loin\n",
    "- **Transformers** : comprendre le mécanisme d'attention et pourquoi il a remplacé les LSTM pour le langage\n",
    "- **Tokenization BPE** : comment les LLMs découpent le texte en sous-mots\n",
    "- **Fine-tuning** : adapter un LLM pré-entraîné à un domaine spécifique (ex: textes juridiques, style littéraire)\n",
    "- **RLHF** : comment les LLMs sont alignés avec les préférences humaines après le pré-entraînement"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
