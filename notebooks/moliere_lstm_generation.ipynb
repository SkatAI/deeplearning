{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]",
    "(https://colab.research.google.com/github/SkatAI/deeplearning/blob/master/notebooks/moliere_lstm_generation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1076412",
   "metadata": {},
   "source": [
    "# Génération de texte avec un LSTM : écrire comme Molière\n",
    "\n",
    "**Objectif** : Entraîner un réseau LSTM character-level à générer du texte dans le style de Molière.\n",
    "\n",
    "**Ce que vous allez apprendre** :\n",
    "- Comment transformer du texte en séquences pour un réseau de neurones\n",
    "- Comment fonctionne un LSTM pour la modélisation de séquences\n",
    "- La différence entre prédiction de séries temporelles et génération de texte (spoiler : c'est le même principe !)\n",
    "- Le rôle de la **temperature** dans la génération\n",
    "- Le lien avec les LLMs modernes (GPT, Claude, Gemini...)\n",
    "\n",
    "**Prérequis** : RNN, LSTM, GRU sur séries temporelles.\n",
    "\n",
    "**Environnement** : Google Colab (free tier, CPU ou GPU T4)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2bae98",
   "metadata": {},
   "source": [
    "## 1. L'intuition : des séries temporelles au texte\n",
    "\n",
    "Vous avez déjà utilisé des LSTM pour prédire des séries temporelles. Le principe était :\n",
    "\n",
    "> Étant donné une séquence de valeurs passées $[x_1, x_2, ..., x_t]$, prédire la prochaine valeur $x_{t+1}$\n",
    "\n",
    "La génération de texte, c'est **exactement la même chose**, sauf qu'au lieu de prédire un nombre, on prédit le **prochain caractère** (ou mot) :\n",
    "\n",
    "> Étant donné `\"Quand l'Amour à vos yeux offre un choi\"` → prédire `\"x\"`\n",
    "\n",
    "C'est le principe fondamental derrière **tous** les modèles de langage actuels, y compris GPT-4 ou Claude. La différence ? L'échelle (milliards de paramètres, architecture Transformer), mais le concept de base est le même : **next token prediction**.\n",
    "\n",
    "Dans ce TD, on va travailler au niveau **caractère** (char-level). Chaque \"token\" est une lettre, un espace, ou un signe de ponctuation. C'est plus simple qu'un tokenizer par mots ou sous-mots, et ça permet d'obtenir des résultats intéressants même avec un petit modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39778624",
   "metadata": {},
   "source": [
    "## 2. Chargement des données\n",
    "\n",
    "On utilise un corpus de pièces de Molière, disponible en CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afde99f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Chargement du dataset\n",
    "url = \"https://skatai.com/data/df_moliere.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(f\"Nombre de répliques : {len(df)}\")\n",
    "print(f\"Colonnes : {list(df.columns)}\")\n",
    "print(f\"Pièces : {df['play_name'].nunique()}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a372b69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On concatène toutes les répliques en un seul texte\n",
    "# On ajoute un retour à la ligne entre chaque réplique pour garder la structure\n",
    "text = \"\\n\".join(df[\"cue\"].dropna().astype(str).values)\n",
    "\n",
    "print(f\"Longueur du texte : {len(text):,} caractères\")\n",
    "print(f\"\\nExtrait :\")\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433bdf81",
   "metadata": {},
   "source": [
    "## 3. Tokenization character-level\n",
    "\n",
    "En char-level, chaque caractère unique du corpus devient un \"token\". Notre vocabulaire est l'ensemble des caractères distincts.\n",
    "\n",
    "Comparez avec les séries temporelles : là-bas vos données étaient déjà numériques. Ici, on doit d'abord **encoder** les caractères en nombres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17db3064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction du vocabulaire\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Taille du vocabulaire : {vocab_size} caractères uniques\")\n",
    "print(f\"\\nCaractères : {''.join(chars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dff1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaires de correspondance caractère <-> index\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Encoder tout le texte en séquence d'entiers\n",
    "text_encoded = np.array([char_to_idx[ch] for ch in text])\n",
    "\n",
    "print(f\"Texte original  : {text[:50]}\")\n",
    "print(f\"Texte encodé    : {text_encoded[:50]}\")\n",
    "print(f\"\\nExemple : '{text[0]}' -> {text_encoded[0]} -> '{idx_to_char[text_encoded[0]]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f50228d",
   "metadata": {},
   "source": [
    "### Comparaison avec les séries temporelles\n",
    "\n",
    "| | Séries temporelles | Texte (char-level) |\n",
    "|---|---|---|\n",
    "| **Token** | Valeur numérique (prix, température...) | Un caractère (lettre, espace, ponctuation) |\n",
    "| **Vocabulaire** | Continu (infini) | Fini (~70-100 caractères) |\n",
    "| **Encoding** | Pas nécessaire | char → index entier → embedding |\n",
    "| **Prédiction** | Régression (valeur continue) | Classification (quel caractère parmi N ?) |\n",
    "| **Loss** | MSE | Cross-entropy |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0c3195",
   "metadata": {},
   "source": [
    "## 4. Création des séquences d'entraînement\n",
    "\n",
    "Comme pour les séries temporelles, on crée des fenêtres glissantes. Pour chaque fenêtre de `SEQ_LENGTH` caractères, la cible est le caractère suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbaaf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# ============================================================\n",
    "# >>> À EXPÉRIMENTER : changez SEQ_LENGTH et observez l'effet\n",
    "# Valeurs suggérées : 40, 60, 100, 150\n",
    "# ============================================================\n",
    "SEQ_LENGTH = 60\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Création des séquences avec tf.data (efficace en mémoire)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(text_encoded)\n",
    "\n",
    "# On crée des fenêtres de SEQ_LENGTH + 1 (input + target)\n",
    "sequences = dataset.window(SEQ_LENGTH + 1, shift=1, drop_remainder=True)\n",
    "sequences = sequences.flat_map(lambda w: w.batch(SEQ_LENGTH + 1))\n",
    "\n",
    "def split_input_target(sequence):\n",
    "    \"\"\"Sépare chaque séquence en input (tous sauf le dernier) et target (tous sauf le premier)\"\"\"\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "# Shuffle, batch, prefetch pour l'entraînement\n",
    "dataset = dataset.shuffle(10000).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Vérification\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(f\"Shape input  : {input_example.shape}  (batch, seq_length)\")\n",
    "    print(f\"Shape target : {target_example.shape}\")\n",
    "    print(f\"\\nExemple (premier élément du batch) :\")\n",
    "    input_chars = ''.join([idx_to_char[i] for i in input_example[0].numpy()])\n",
    "    target_chars = ''.join([idx_to_char[i] for i in target_example[0].numpy()])\n",
    "    print(f\"  Input  : '{input_chars}'\")\n",
    "    print(f\"  Target : '{target_chars}'\")\n",
    "    print(f\"\\n  → Le target est l'input décalé d'un caractère vers la droite.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84718805",
   "metadata": {},
   "source": [
    "**Remarquez** : le target est simplement l'input décalé d'une position. Pour chaque position $t$, le modèle doit prédire le caractère en position $t+1$. C'est exactement le même principe que le \"sliding window\" des séries temporelles.\n",
    "\n",
    "> **Question pour vous** : que se passe-t-il si on augmente `SEQ_LENGTH` ? Quel est le compromis ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069ae5f0",
   "metadata": {},
   "source": [
    "## 5. Construction du modèle LSTM\n",
    "\n",
    "L'architecture est simple :\n",
    "1. **Embedding** : transforme chaque index de caractère en un vecteur dense (comme un word2vec, mais pour des caractères)\n",
    "2. **LSTM** : apprend les dépendances séquentielles\n",
    "3. **Dense + softmax** : prédit une distribution de probabilité sur les `vocab_size` caractères possibles\n",
    "\n",
    "C'est un problème de **classification** à chaque pas de temps : parmi les ~80 caractères possibles, lequel vient ensuite ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03fb439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# ============================================================\n",
    "# >>> À EXPÉRIMENTER : modifiez ces hyperparamètres\n",
    "# EMBEDDING_DIM : 64, 128, 256\n",
    "# LSTM_UNITS : 128, 256, 512\n",
    "# Ajoutez un 2e layer LSTM (return_sequences=True sur le premier)\n",
    "# ============================================================\n",
    "EMBEDDING_DIM = 128\n",
    "LSTM_UNITS = 256\n",
    "\n",
    "model = Sequential([\n",
    "    # Embedding : chaque caractère (index) -> vecteur dense\n",
    "    Embedding(vocab_size, EMBEDDING_DIM, input_length=SEQ_LENGTH),\n",
    "\n",
    "    # LSTM : capture les dépendances dans la séquence\n",
    "    LSTM(LSTM_UNITS, return_sequences=False),\n",
    "\n",
    "    # Dropout pour la régularisation\n",
    "    Dropout(0.2),\n",
    "\n",
    "    # Couche de sortie : probabilité pour chaque caractère du vocabulaire\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d2fe6f",
   "metadata": {},
   "source": [
    "### Comprendre l'architecture\n",
    "\n",
    "- **Embedding(vocab_size, 128)** : Le modèle apprend une représentation vectorielle pour chaque caractère. Les caractères qui apparaissent dans des contextes similaires (ex: voyelles entre elles) auront des embeddings proches.\n",
    "\n",
    "- **LSTM(256)** : La couche LSTM lit la séquence caractère par caractère et maintient un état interne (cell state + hidden state). C'est cet état qui \"résume\" ce qui a été lu et permet de prédire la suite.\n",
    "\n",
    "- **Dense(vocab_size, softmax)** : La sortie est un vecteur de probabilités. Si le vocabulaire a 80 caractères, on obtient 80 probabilités qui somment à 1.\n",
    "\n",
    "> **Question** : pourquoi utilise-t-on `sparse_categorical_crossentropy` et pas `categorical_crossentropy` ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38db9f64",
   "metadata": {},
   "source": [
    "## 6. Entraînement\n",
    "\n",
    "On entraîne sur quelques epochs. Sur free Colab (T4 GPU ou CPU), ça devrait prendre quelques minutes par epoch.\n",
    "\n",
    "**Astuce** : Activez le GPU dans Colab via `Runtime > Change runtime type > T4 GPU` pour accélérer l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a192d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# >>> À EXPÉRIMENTER : nombre d'epochs\n",
    "# 5 epochs  : le modèle commence à apprendre la structure\n",
    "# 15 epochs : résultats intéressants\n",
    "# 30+ epochs : meilleure qualité (mais plus long)\n",
    "# ============================================================\n",
    "EPOCHS = 15\n",
    "\n",
    "history = model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec26cad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(history.history['loss'])\n",
    "ax1.set_title('Loss par epoch')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(history.history['accuracy'])\n",
    "ax2.set_title('Accuracy par epoch')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nLoss finale : {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Accuracy finale : {history.history['accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2073315",
   "metadata": {},
   "source": [
    "## 7. Génération de texte et temperature\n",
    "\n",
    "Maintenant, on utilise le modèle pour **générer** du texte. Le processus est autorégressif :\n",
    "\n",
    "1. On donne une séquence de départ (seed)\n",
    "2. Le modèle prédit une distribution de probabilité sur le prochain caractère\n",
    "3. On **échantillonne** un caractère selon cette distribution\n",
    "4. On ajoute ce caractère à la séquence et on recommence\n",
    "\n",
    "### Le rôle de la temperature\n",
    "\n",
    "La **temperature** contrôle la \"créativité\" de la génération en modifiant la distribution de probabilité avant l'échantillonnage :\n",
    "\n",
    "$$p_i = \\frac{\\exp(\\log(p_i) / T)}{\\sum_j \\exp(\\log(p_j) / T)}$$\n",
    "\n",
    "- **T → 0** : le modèle choisit presque toujours le caractère le plus probable → texte répétitif, \"sûr\"\n",
    "- **T = 1** : distribution non modifiée → équilibre\n",
    "- **T > 1** : distribution plus uniforme → texte plus \"créatif\", plus de surprises, mais aussi plus d'erreurs\n",
    "\n",
    "C'est exactement le même paramètre que vous retrouvez dans les API de GPT, Claude, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07edf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, num_generate=300, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Génère du texte caractère par caractère.\n",
    "\n",
    "    Args:\n",
    "        model: le modèle LSTM entraîné\n",
    "        start_string: texte de départ (seed)\n",
    "        num_generate: nombre de caractères à générer\n",
    "        temperature: contrôle la créativité (0.2 = conservateur, 1.5 = créatif)\n",
    "    \"\"\"\n",
    "    # Encoder le texte de départ\n",
    "    input_eval = [char_to_idx[ch] for ch in start_string]\n",
    "\n",
    "    # S'assurer que la séquence fait exactement SEQ_LENGTH\n",
    "    if len(input_eval) < SEQ_LENGTH:\n",
    "        # Padding à gauche avec des espaces\n",
    "        pad = [char_to_idx[' ']] * (SEQ_LENGTH - len(input_eval))\n",
    "        input_eval = pad + input_eval\n",
    "    else:\n",
    "        input_eval = input_eval[-SEQ_LENGTH:]\n",
    "\n",
    "    generated = list(start_string)\n",
    "\n",
    "    for _ in range(num_generate):\n",
    "        input_array = np.array([input_eval])\n",
    "\n",
    "        # Prédiction : distribution de probabilité sur le vocabulaire\n",
    "        predictions = model.predict(input_array, verbose=0)[0]\n",
    "\n",
    "        # Application de la temperature\n",
    "        predictions = np.log(predictions + 1e-8) / temperature\n",
    "        predictions = np.exp(predictions)\n",
    "        predictions = predictions / np.sum(predictions)\n",
    "\n",
    "        # Échantillonnage selon la distribution\n",
    "        predicted_id = np.random.choice(len(predictions), p=predictions)\n",
    "\n",
    "        # Ajouter le caractère généré et décaler la fenêtre\n",
    "        generated.append(idx_to_char[predicted_id])\n",
    "        input_eval = input_eval[1:] + [predicted_id]\n",
    "\n",
    "    return ''.join(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52543889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# >>> À EXPÉRIMENTER : changez la temperature et le seed\n",
    "# ============================================================\n",
    "\n",
    "seed_text = \"Quand l'Amour\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "for temp in [0.2, 0.5, 0.8, 1.0, 1.3]:\n",
    "    print(f\"\\n--- Temperature = {temp} ---\\n\")\n",
    "    result = generate_text(model, seed_text, num_generate=200, temperature=temp)\n",
    "    print(result)\n",
    "    print()\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dad4c0",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "> **Exercice** : Comparez les textes générés aux différentes temperatures.\n",
    ">\n",
    "> 1. À temperature basse (0.2), que remarquez-vous ? Le texte est-il varié ?\n",
    "> 2. À temperature haute (1.3), la qualité diminue-t-elle ? Pourquoi ?\n",
    "> 3. Quelle temperature vous semble produire le meilleur compromis ?\n",
    ">\n",
    "> **Essayez aussi** avec différents textes de départ (seed). Le modèle génère-t-il différemment selon le contexte initial ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3581332c",
   "metadata": {},
   "source": [
    "## 8. À vous de jouer !\n",
    "\n",
    "### Expérience 1 : Impact de la longueur de séquence\n",
    "\n",
    "Revenez à la section 4 et changez `SEQ_LENGTH`. Essayez 40, 100, et 150. Réentraînez et comparez la qualité du texte généré.\n",
    "\n",
    "> Plus la séquence est longue, plus le modèle a de contexte, mais l'entraînement est plus lent et nécessite plus de mémoire.\n",
    "\n",
    "### Expérience 2 : Architecture du modèle\n",
    "\n",
    "Modifiez le modèle dans la section 5 :\n",
    "- Essayez un modèle plus petit (LSTM 128 units) vs plus grand (LSTM 512)\n",
    "- Ajoutez une deuxième couche LSTM :\n",
    "\n",
    "```python\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, EMBEDDING_DIM, input_length=SEQ_LENGTH),\n",
    "    LSTM(256, return_sequences=True),   # return_sequences=True pour empiler\n",
    "    LSTM(256, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "- Remplacez le LSTM par un GRU. Le résultat change-t-il ?\n",
    "\n",
    "### Expérience 3 : Sous-corpus\n",
    "\n",
    "Essayez d'entraîner sur une seule pièce. Le style est-il différent ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5332c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple : entraîner sur une seule pièce\n",
    "# Décommentez et adaptez :\n",
    "\n",
    "# piece = \"tartuffe\"  # ou \"misanthrope\", etc.\n",
    "# df_piece = df[df['play_name'] == piece]\n",
    "# text_piece = \"\\n\".join(df_piece[\"cue\"].dropna().astype(str).values)\n",
    "# print(f\"Pièce : {piece}, {len(text_piece):,} caractères\")\n",
    "# print(f\"Pièces disponibles : {sorted(df['play_name'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a42f95",
   "metadata": {},
   "source": [
    "## 9. Bonus : comparaison avec un modèle Hugging Face\n",
    "\n",
    "Pour mettre en perspective, comparons notre petit LSTM avec un modèle de langage pré-entraîné. On utilise un modèle GPT-2 (le plus petit, 124M paramètres) pour générer du texte à partir du même seed.\n",
    "\n",
    "Notre LSTM a quelques centaines de milliers de paramètres et a été entraîné sur quelques centaines de Ko de texte. GPT-2 \"small\" a 124 millions de paramètres et a été entraîné sur des milliards de mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fbe6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation si nécessaire (déjà installé sur Colab en général)\n",
    "# !pip install transformers -q\n",
    "\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "# Charger un modèle GPT-2 (le plus petit, ~500Mo)\n",
    "# On utilise un modèle multilingue petit pour le français\n",
    "try:\n",
    "    generator = pipeline('text-generation', model='gpt2', device=-1)  # CPU\n",
    "    set_seed(42)\n",
    "\n",
    "    seed_text_hf = \"Quand l'Amour\"\n",
    "\n",
    "    result_hf = generator(\n",
    "        seed_text_hf,\n",
    "        max_length=150,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.8,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    print(\"--- GPT-2 (124M params, entraîné sur du texte anglais principalement) ---\")\n",
    "    print(result_hf[0]['generated_text'])\n",
    "    print()\n",
    "    print(\"--- Notre LSTM (entraîné sur Molière uniquement) ---\")\n",
    "    print(generate_text(model, seed_text_hf, num_generate=150, temperature=0.8))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors du chargement de GPT-2 : {e}\")\n",
    "    print(\"Ce n'est pas grave, l'essentiel est notre modèle LSTM !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694b9946",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "> **Question** : GPT-2 est entraîné principalement en anglais. Comment se comporte-t-il sur un prompt en français ? Pourquoi ?\n",
    ">\n",
    "> **Question** : Notre LSTM produit du texte \"à la Molière\" car il n'a vu que ça. C'est à la fois sa force (spécialisation) et sa limite (pas de connaissance générale). Comment les LLMs modernes gèrent-ils ce compromis ?\n",
    ">\n",
    "> **Pour aller plus loin** : essayez avec un modèle comme `\"asi/gpt-fr-cased-small\"` (GPT-2 entraîné sur du français) si disponible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2943c11",
   "metadata": {},
   "source": [
    "## 10. Récapitulatif et liens avec les LLMs\n",
    "\n",
    "### Ce qu'on a fait\n",
    "1. **Tokenization** : transformer du texte en séquence d'entiers (ici char-level)\n",
    "2. **Séquences** : créer des fenêtres glissantes input/target (comme en séries temporelles)\n",
    "3. **Modèle** : Embedding → LSTM → Dense (classification sur le vocabulaire)\n",
    "4. **Génération** : prédiction autoregressive + sampling avec temperature\n",
    "\n",
    "### Le pont vers les LLMs modernes\n",
    "\n",
    "| | Notre LSTM | GPT / Claude / Gemini |\n",
    "|---|---|---|\n",
    "| **Tokenization** | Character-level (~80 tokens) | Subword (BPE, ~50K-100K tokens) |\n",
    "| **Architecture** | LSTM | Transformer (attention) |\n",
    "| **Contexte** | 60-100 caractères | 8K - 1M+ tokens |\n",
    "| **Paramètres** | ~300K | 7B - 1000B+ |\n",
    "| **Entraînement** | Un corpus (~500 Ko) | Internet (~To de texte) |\n",
    "| **Principe** | Next token prediction | Next token prediction |\n",
    "\n",
    "Le principe fondamental est **identique** : prédire le prochain token. Ce qui change, c'est l'échelle, l'architecture (attention vs récurrence), et la quantité de données.\n",
    "\n",
    "### Pour aller plus loin\n",
    "- **Transformers** : comprendre le mécanisme d'attention et pourquoi il a remplacé les LSTM pour le langage\n",
    "- **Tokenization BPE** : comment les LLMs découpent le texte en sous-mots\n",
    "- **Fine-tuning** : adapter un LLM pré-entraîné à un domaine spécifique (ex: textes juridiques, style littéraire)\n",
    "- **RLHF** : comment les LLMs sont alignés avec les préférences humaines après le pré-entraînement"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
