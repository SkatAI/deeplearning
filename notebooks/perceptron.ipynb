{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Perceptron\n",
        "\n",
        "Dans ce notebook nous allons implementer l'algorithme du perceptron\n",
        "avec 2 temps: sans puis avec le learning rate\n",
        "\n",
        "Nous observerons\n",
        "- la convergence du perceptron sur les OR et sur le XOR\n",
        "- la dépendence aux valeurs d'initialisation\n",
        "- l'impact du learning rate\n"
      ],
      "metadata": {
        "id": "OD_oZpuWh0av"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## L'agorithme du perceptron\n",
        "\n",
        "Soit X une matrice de N echantillons de taille p et y le vecteur de classification binaire à valeurd dans [0, 1]\n",
        "\n",
        "L'algorithme va trouver les coefficients w et le biais b tels que\n",
        "\n",
        "H(w.X^T + b) = y\n",
        "\n",
        "ou H est la fonction de heavyside :\n",
        "\n",
        "H(x) = 0 si x < 0 1 sinon\n",
        "\n",
        "\n",
        "\n",
        "1. initialisation des valeurs\n",
        "    ```\n",
        "    w = np.zeros(p)\n",
        "    b = 0\n",
        "    ```\n",
        "\n",
        "2. a chaque iteration :\n",
        "    - echantillon aleatoire de X et y : xi et yi\n",
        "    - calcul de y_hat = (w.xi + b)\n",
        "    - si y_hat != yi : maj des coefficients et du biais\n",
        "    ```\n",
        "    w += (yi - y_hat) * xi\n",
        "    b += (yi - y_hat)\n",
        "    ```\n",
        "\n"
      ],
      "metadata": {
        "id": "i9A2vurViX0W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYVp_b0rhXo6"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perceptron sur le OR"
      ],
      "metadata": {
        "id": "0yRygV5HoaIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input data\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "# Labels OR\n",
        "y = np.array([0, 1, 1, 1])\n",
        "\n",
        "# initialisation des coefficients et du biais\n",
        "w = np.zeros(X.shape[1])\n",
        "b = 0\n",
        "# Nombre d'iterations\n",
        "n_iters = 20\n"
      ],
      "metadata": {
        "id": "AI4gw0qCk8RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yPtUn2TfonLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for n in range(n_iters):\n",
        "    # Echantillonage\n",
        "    idx = np.random.randint(0, len(y))\n",
        "    xi = X[idx]\n",
        "    target = y[idx]\n",
        "\n",
        "    # Combinaison linéaire des coefficients et du biais\n",
        "    sample_output = np.dot(xi, w) + b\n",
        "    # Prediction avec la fonction de Heavyside\n",
        "    # seuil = 0\n",
        "    y_pred = 1 if sample_output >= 0 else 0\n",
        "\n",
        "    # Mise a jour des coefficients et du biais\n",
        "    # si la target == y_pred alors update est nulle\n",
        "    update = target - y_pred\n",
        "    w += update * xi\n",
        "    b += update\n",
        "\n",
        "    print(f\"Iteration {n+1}: Target={target}, Input={xi}, Weights={w}, Bias={b}, Loss={loss}\")\n",
        "    # test de convergence\n",
        "    # on verifie si le percetron classifie bien tous les X\n",
        "    full_output = np.dot(X, np.transpose(w))  + b\n",
        "    # heavyside\n",
        "    full_output = np.where(full_output < 0, 0, 1)\n",
        "\n",
        "    # combien d'échantillons ont été mal classé ?\n",
        "    loss = np.sum(np.abs(full_output-y))\n",
        "    if loss == 0:\n",
        "        print(f\"-- convergence ! {n} iterations\")\n",
        "        break;\n",
        "\n",
        "print(\"Final Weights:\", w)\n",
        "print(\"Final Bias:\", b)\n"
      ],
      "metadata": {
        "id": "mxMzpZ1zlI3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A vous\n",
        "\n",
        "- executez le code plusieurs fois et observez le nombre d'itérations nécessaires pour converger\n",
        "- modifiez les valeurs d'initialisations de w et b\n",
        "\n"
      ],
      "metadata": {
        "id": "MmnEhL60l2rV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XOR\n",
        "Le perceptron apprend OR mais peut il apprendre XOR (non linéairement séparable )"
      ],
      "metadata": {
        "id": "dEoVo7-Pmd0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XOR\n",
        "# X est pareil\n",
        "y = np.array([0, 1, 1, 0])\n"
      ],
      "metadata": {
        "id": "oyixZgIBll23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A vous\n",
        "\n",
        "Faites tourner le perceptron sur XOR, y a t il convergence ?"
      ],
      "metadata": {
        "id": "eoRobMjMmrHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning rate\n",
        "\n",
        "Le learning rate est un paramètres important en machine learning qui règle la quantité de modification des coefficients à chaque itération.\n",
        "\n",
        "pour ajouter le learning rate _alpha_ , il suffit de modifier la ligne\n",
        "\n",
        "```\n",
        "    update = target - y_pred\n",
        "```\n",
        "en\n",
        "```\n",
        "    update = alpha * (target - y_pred)\n",
        "```\n",
        "où _alpha_  <= 1\n",
        "\n",
        "Le code du perceptron devient"
      ],
      "metadata": {
        "id": "hhGw2cw2mv9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input data\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "# Labels OR\n",
        "y = np.array([0, 1, 1, 1])\n",
        "# coeffs et biais\n",
        "w = np.zeros(X.shape[1])\n",
        "b = 0\n",
        "# Number of iterations\n",
        "n_iters = 20\n",
        "\n",
        "# learning rate\n",
        "alpha = 0.5\n"
      ],
      "metadata": {
        "id": "RHz_Y_70nrAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n in range(n_iters):\n",
        "    # Echantillonage\n",
        "    idx = np.random.randint(0, len(y))\n",
        "    xi = X[idx]\n",
        "    target = y[idx]\n",
        "\n",
        "    # combinaison linéaire des coefficients et du biais\n",
        "    sample_output = np.dot(xi, w) + b\n",
        "    # Prediction avec la fonction de Heavyside\n",
        "    y_pred = 1 if sample_output >= 0 else 0\n",
        "\n",
        "    # Mise a jour des coefficients et du biais\n",
        "    update = alpha * (target - y_pred)\n",
        "    w += update * xi\n",
        "    b += update\n",
        "    print(f\"Iteration {n+1}: Target={target}, Input={xi}, Weights={w}, Bias={b}, Loss={loss}\")\n",
        "\n",
        "    # test de convergence\n",
        "    full_output = np.dot(X, np.transpose(w))  + b\n",
        "    full_output = np.where(full_output < 0, 0, 1)\n",
        "    loss = np.sum(np.abs(full_output-y))\n",
        "    if loss == 0:\n",
        "        print(f\"-- convergence ! {n} iterations\")\n",
        "        break;\n",
        "\n",
        "print(\"Final Weights:\", w)\n",
        "print(\"Final Bias:\", b)\n"
      ],
      "metadata": {
        "id": "taY_DzamsBtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A vous\n",
        "\n",
        "Pour alpha = 1 on retrouve la premiere version du perceptron\n",
        "\n",
        "faites tourner le code pour des valeurs de alpha variant de tres petit (0.01 ou moins) à proche de 1\n",
        "\n",
        "Quel est l'impact du learning rate ?"
      ],
      "metadata": {
        "id": "eqapC_gynzCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Blobs\n",
        "\n",
        "Prenons maintenant un dataset plus complexe avec la fonction `make_blob` de scikit-learn\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html"
      ],
      "metadata": {
        "id": "Ngo7f1tCoNB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_blobs(n_samples=30, centers=2, n_features=2, random_state=80, cluster_std = 1.0)\n",
        "print(X.shape)\n",
        "print(y)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', label=['Class 0', 'Class 1'])\n",
        "\n"
      ],
      "metadata": {
        "id": "fkBhDbRboL3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Faites tourner le perceptron sur ce dataset"
      ],
      "metadata": {
        "id": "P-dZVVudqV8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=80, cluster_std = 1.0)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm')\n",
        "\n",
        "w = np.zeros(X.shape[1])\n",
        "b = 0\n",
        "# Number of iterations\n",
        "n_iters = 20\n",
        "\n",
        "# learning rate\n",
        "alpha = 0.1\n",
        "\n",
        "for n in range(n_iters):\n",
        "    # Echantillonage\n",
        "    idx = np.random.randint(0, len(y))\n",
        "    xi = X[idx]\n",
        "    target = y[idx]\n",
        "\n",
        "    # combinaison linéaire des coefficients et du biais\n",
        "    sample_output = np.dot(xi, w) + b\n",
        "    # Prediction avec la fonction de Heavyside\n",
        "    y_pred = 1 if sample_output >= 0 else 0\n",
        "\n",
        "    # Mise a jour des coefficients et du biais\n",
        "    update = alpha * (target - y_pred)\n",
        "    w += update * xi\n",
        "    b += update\n",
        "    print(f\"Iteration {n+1}: Target={target}, Input={xi}, Weights={w}, Bias={b}, Loss={loss}\")\n",
        "\n",
        "    # test de convergence\n",
        "    full_output = np.dot(X, np.transpose(w))  + b\n",
        "    full_output = np.where(full_output < 0, 0, 1)\n",
        "    loss = np.sum(np.abs(full_output-y))\n",
        "    if loss == 0:\n",
        "        print(f\"-- convergence ! {n} iterations\")\n",
        "        break;\n",
        "\n",
        "print(\"Final Weights:\", w)\n",
        "print(\"Final Bias:\", b)"
      ],
      "metadata": {
        "id": "CuKQGZI8pEEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A vous\n",
        "Que se passe t il quand on augmente la variance de X ?\n",
        "\n",
        "cluster_std = 2.0"
      ],
      "metadata": {
        "id": "WKVdHBisrTt7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q6XoN5K_qx3L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}