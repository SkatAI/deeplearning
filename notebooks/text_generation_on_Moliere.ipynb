{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !wget https://www.gutenberg.org/cache/epub/50173/pg50173.txt\n",
        "# !mv pg50173.txt moliere.txt"
      ],
      "metadata": {
        "id": "8dn_unVaFGHy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade tensorflow-keras\n"
      ],
      "metadata": {
        "id": "GmcBdTdbkQi9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6JAawfxkV9x",
        "outputId": "3ae9e3f4-b640-4cc0-8e61-ca32a4ac5c4a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
      ],
      "metadata": {
        "id": "PfT3LbWxlqRc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm french.txt\n",
        "# !rm dom-juan.txt.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGjDIJk8buyb",
        "outputId": "63c608c5-7a88-492d-9967-b8ed921adafe"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'french.txt': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# !wget https://raw.githubusercontent.com/SkatAI/skatai_deeplearning/master/data/moliere.txt\n",
        "!wget https://raw.githubusercontent.com/SkatAI/skatai_deeplearning/master/data/le-horla.txt\n",
        "\n",
        "!mv le-horla.txt french.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ha40Y0Jbbcpy",
        "outputId": "8bb615eb-ac10-4108-bfa8-a4b971662d6f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-09 15:44:41--  https://raw.githubusercontent.com/SkatAI/skatai_deeplearning/master/data/le-horla.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 256796 (251K) [text/plain]\n",
            "Saving to: ‘le-horla.txt’\n",
            "\n",
            "\rle-horla.txt          0%[                    ]       0  --.-KB/s               \rle-horla.txt        100%[===================>] 250.78K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-06-09 15:44:41 (6.44 MB/s) - ‘le-horla.txt’ saved [256796/256796]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -al"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d67bVy56FII4",
        "outputId": "f799a6c0-0a57-4e0e-d8d7-da47c7953f1c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 872\n",
            "drwxr-xr-x 1 root root   4096 Jun  9 15:44 .\n",
            "drwxr-xr-x 1 root root   4096 Jun  9 15:22 ..\n",
            "drwxr-xr-x 4 root root   4096 Jun  6 14:15 .config\n",
            "-rw-r--r-- 1 root root 256796 Jun  9 15:44 french.txt\n",
            "-rw-r--r-- 1 root root 616806 Jun  9 15:22 moliere.txt\n",
            "drwxr-xr-x 1 root root   4096 Jun  6 14:21 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 20 french.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gLWmhDzUJ7C",
        "outputId": "7503d563-22e8-4ea6-c56b-1ccff65a166c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿GUY DE MAUPASSANT\r\n",
            "\r\n",
            "Le Horla\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "1887\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "LE HORLA\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "_8 mai._--Quelle journée admirable! J'ai passé toute la matinée étendu sur\r\n",
            "l'herbe, devant ma maison, sous l'énorme platane qui la couvre, l'abrite et\r\n",
            "l'ombrage tout entière. J'aime ce pays, et j'aime y vivre parce que j'y ai\r\n",
            "mes racines, ces profondes et délicates racines, qui attachent un homme à\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import CamembertTokenizer, TFCamembertModel\n",
        "\n",
        "# Load the text data\n",
        "with open('french.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n"
      ],
      "metadata": {
        "id": "NcWkV76JGCZ7"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nombre de lignes\n",
        "len(text.split('\\n'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r671KQkYVmsO",
        "outputId": "98f61b54-8f55-4b71-eff0-e0fbb9ea26c0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5688"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[200: 400])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtR8xb_4Nvwo",
        "outputId": "e47c0b84-cb4e-4545-b1a7-f211adc26c29"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e et\n",
            "l'ombrage tout entière. J'aime ce pays, et j'aime y vivre parce que j'y ai\n",
            "mes racines, ces profondes et délicates racines, qui attachent un homme à\n",
            "la terre où sont nés et morts ses aïeux, qui l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and prepare data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YHrfCax4isAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the text data\n",
        "with open('french.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Tokenize the text using Keras Tokenizer\n",
        "vocab_size = 2000  # Adjust based on your tokenizer's vocabulary size\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts([text])\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences([text])[0]\n",
        "SEQ_LENGTH = 40  # Adjust based on your input sequence length\n",
        "\n",
        "\n",
        "# Prepare input and target sequences\n",
        "input_sequences = []\n",
        "target_sequences = []\n",
        "\n",
        "for i in range(len(sequences) - SEQ_LENGTH):\n",
        "    input_sequences.append(sequences[i:i+SEQ_LENGTH])\n",
        "    target_sequences.append(sequences[i+SEQ_LENGTH])\n",
        "\n",
        "input_sequences = np.array(input_sequences)\n",
        "target_sequences = np.array(target_sequences)\n",
        "target_sequences = np.expand_dims(target_sequences, axis=-1)  # Ensure target sequences have the right shape\n"
      ],
      "metadata": {
        "id": "KJv-DVsriqBE"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "35upEdjGjKpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, LSTM, Dense\n",
        "\n",
        "embedding_dim = 128\n",
        "units = 128\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=SEQ_LENGTH),\n",
        "    LSTM(units, return_sequences=True),\n",
        "    LSTM(units),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBedIdIijMYg",
        "outputId": "74b454a8-14a5-470c-d20e-8cb05e5b5899"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 40, 128)           256000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 40, 128)           131584    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 128)               131584    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2000)              258000    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 777168 (2.96 MB)\n",
            "Trainable params: 777168 (2.96 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(input_sequences, target_sequences, epochs=20, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgKOXBSFjRKl",
        "outputId": "a8d90c83-0fca-46bd-ae80-a806b1a3f131"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "655/655 [==============================] - 95s 140ms/step - loss: 5.7396\n",
            "Epoch 2/20\n",
            "655/655 [==============================] - 80s 122ms/step - loss: 5.4467\n",
            "Epoch 3/20\n",
            "655/655 [==============================] - 78s 120ms/step - loss: 5.2795\n",
            "Epoch 4/20\n",
            "655/655 [==============================] - 83s 127ms/step - loss: 5.1780\n",
            "Epoch 5/20\n",
            "655/655 [==============================] - 80s 122ms/step - loss: 5.0791\n",
            "Epoch 6/20\n",
            "655/655 [==============================] - 78s 119ms/step - loss: 4.9870\n",
            "Epoch 7/20\n",
            "655/655 [==============================] - 80s 122ms/step - loss: 4.8766\n",
            "Epoch 8/20\n",
            "655/655 [==============================] - 79s 121ms/step - loss: 4.7730\n",
            "Epoch 9/20\n",
            "655/655 [==============================] - 79s 121ms/step - loss: 4.6767\n",
            "Epoch 10/20\n",
            "655/655 [==============================] - 83s 126ms/step - loss: 4.5859\n",
            "Epoch 11/20\n",
            "655/655 [==============================] - 80s 122ms/step - loss: 4.5003\n",
            "Epoch 12/20\n",
            "655/655 [==============================] - 77s 118ms/step - loss: 4.4199\n",
            "Epoch 13/20\n",
            "655/655 [==============================] - 80s 122ms/step - loss: 4.3413\n",
            "Epoch 14/20\n",
            "655/655 [==============================] - 82s 125ms/step - loss: 4.2684\n",
            "Epoch 15/20\n",
            "655/655 [==============================] - 82s 125ms/step - loss: 4.2014\n",
            "Epoch 16/20\n",
            "655/655 [==============================] - 81s 124ms/step - loss: 4.1346\n",
            "Epoch 17/20\n",
            "655/655 [==============================] - 81s 123ms/step - loss: 4.0731\n",
            "Epoch 18/20\n",
            "655/655 [==============================] - 85s 129ms/step - loss: 4.0108\n",
            "Epoch 19/20\n",
            "655/655 [==============================] - 81s 123ms/step - loss: 3.9499\n",
            "Epoch 20/20\n",
            "655/655 [==============================] - 81s 124ms/step - loss: 3.8918\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e7fed717280>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, seed_text, seq_length=SEQ_LENGTH, max_length=100, temperature=1.0):\n",
        "    generated_text = seed_text\n",
        "    for _ in range(max_length):\n",
        "        # Tokenize the input text\n",
        "        input_tokens = tokenizer.texts_to_sequences([generated_text])[0]\n",
        "\n",
        "        # Ensure the input sequence length is equal to SEQ_LENGTH\n",
        "        if len(input_tokens) < seq_length:\n",
        "            input_tokens = [0] * (seq_length - len(input_tokens)) + input_tokens\n",
        "        else:\n",
        "            input_tokens = input_tokens[-seq_length:]\n",
        "\n",
        "        # Convert to numpy array and reshape for model input\n",
        "        input_tokens = np.array(input_tokens).reshape(1, -1)\n",
        "\n",
        "        # Predict next token\n",
        "        predictions = model.predict(input_tokens)[0]\n",
        "        predictions = predictions / temperature\n",
        "\n",
        "        # Sample the next token\n",
        "        next_token_id = tf.random.categorical(tf.math.log([predictions]), num_samples=1).numpy()[0][0]\n",
        "\n",
        "        # Append next token to the generated text\n",
        "        generated_text += ' ' + tokenizer.index_word[next_token_id]\n",
        "\n",
        "        if tokenizer.index_word[next_token_id] == '<end>':\n",
        "            break\n",
        "\n",
        "    return generated_text\n",
        "\n"
      ],
      "metadata": {
        "id": "tYi1xzxajXRI"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"Bonjour, comment allez-vous?\"\n",
        "new_text = generate_text(model, tokenizer, seed_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_yR2x5yjZY0",
        "outputId": "65cbbdc9-60ea-4e72-939c-d4124b9c0af7"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 279ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(new_text.replace('<OOV>', ''))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIh8kNYvpuKw",
        "outputId": "609f34fb-89cd-4726-d7fe-2acd61094108"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bonjour, comment allez-vous? rappelez chercher une surprise un  droit de passer pour l'air  qui frôlant le amoureux il  quand les dents devenaient guide où resta encore quand s'il eut  dans une peau est m'sieu  de rester d'en pas nez affreusement tous les bras rose et  puis je me couche des pierres du service c'est imparfait qu'il était étendu les légendes un  clair d'une fois oh monsieur il reprit une lettre au soeur cria  il était fini huit fois à ça nous ne avait ou maigre haut nous  aussi mon jour qu'une main pour nous\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8dghhFVfjY42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# legacy"
      ],
      "metadata": {
        "id": "UodjEU3winjS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vOC_iJoHim7-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text using CamembertTokenizer\n",
        "# tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
        "# tokens = tokenizer(text, return_tensors='tf', truncation=True, padding='max_length', max_length=512)['input_ids']\n",
        "\n",
        "# # Convert tokens to a numpy array\n",
        "# tokens = tokens.numpy().flatten()\n",
        "\n",
        "# # Define sequence length\n",
        "# SEQ_LENGTH = 20\n",
        "\n",
        "# # Prepare input and target sequences\n",
        "# input_sequences = []\n",
        "# target_sequences = []\n",
        "\n",
        "# for i in range(len(tokens) - SEQ_LENGTH):\n",
        "#     input_sequences.append(tokens[i:i+SEQ_LENGTH])\n",
        "#     target_sequences.append(tokens[i+SEQ_LENGTH])\n",
        "\n",
        "# # Convert to numpy arrays\n",
        "# input_sequences = np.array(input_sequences)\n",
        "# target_sequences = np.array(target_sequences)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dEQxqttQWSAC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Camembert model for embeddings\n",
        "# camembert_model = TFCamembertModel.from_pretrained(\"camembert-base\")\n",
        "\n",
        "# # Function to get embeddings in batches\n",
        "# def get_embeddings_in_batches(input_sequences, batch_size=32):\n",
        "#     embeddings = []\n",
        "#     for i in range(0, len(input_sequences), batch_size):\n",
        "#         batch_sequences = input_sequences[i:i+batch_size]\n",
        "#         batch_embeddings = camembert_model(tf.convert_to_tensor(batch_sequences)).last_hidden_state\n",
        "#         embeddings.append(batch_embeddings.numpy())\n",
        "#     return np.concatenate(embeddings, axis=0)\n",
        "\n",
        "# # Get embeddings for input sequences\n",
        "# input_embeddings = get_embeddings_in_batches(input_sequences, batch_size=16)\n"
      ],
      "metadata": {
        "id": "WUAMZINrGQRp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# # Reshape target sequences for training\n",
        "# target_sequences = np.expand_dims(target_sequences, axis=-1)\n",
        "\n",
        "# # Define the RNN model\n",
        "# model = Sequential([\n",
        "#     LSTM(256, input_shape=(SEQ_LENGTH, input_embeddings.shape[-1]), return_sequences=True),\n",
        "#     LSTM(256),\n",
        "#     Dense(tokenizer.vocab_size, activation='softmax')\n",
        "# ])\n",
        "\n",
        "# # Compile the model\n",
        "# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n"
      ],
      "metadata": {
        "id": "PM_bG3xbGbZv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the model\n",
        "# model.fit(input_embeddings, target_sequences, epochs=200, batch_size=64)\n"
      ],
      "metadata": {
        "id": "TvrP05fFGdYq",
        "collapsed": true
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "x7wIgW83Yq6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# temperature = 2.0"
      ],
      "metadata": {
        "id": "NOGyarZpYr3J"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def generate_text(model, tokenizer, seed_text, max_length=20):\n",
        "#     generated_text = seed_text\n",
        "\n",
        "#     for _ in range(max_length):\n",
        "#         # Tokenize the input text\n",
        "#         input_tokens = tokenizer(generated_text, return_tensors='tf')['input_ids']\n",
        "#         input_tokens = input_tokens.numpy().flatten()\n",
        "\n",
        "\n",
        "#         if len(input_tokens) < SEQ_LENGTH:\n",
        "#             # Pad the sequence if it's shorter than SEQ_LENGTH\n",
        "#             input_tokens = np.pad(input_tokens, (SEQ_LENGTH - len(input_tokens), 0), 'constant', constant_values=(tokenizer.pad_token_id, tokenizer.pad_token_id))\n",
        "#         else:\n",
        "#             # Truncate the sequence if it's longer than SEQ_LENGTH\n",
        "#             input_tokens = input_tokens[-SEQ_LENGTH:]\n",
        "\n",
        "#         # Prepare input embeddings\n",
        "#         input_sequences = input_tokens[-SEQ_LENGTH:]\n",
        "#         input_embeddings = get_embeddings_in_batches([input_sequences], batch_size=1)\n",
        "\n",
        "#         # Predict next token\n",
        "#         predictions = model.predict(input_embeddings)[0]\n",
        "#         predictions = predictions / temperature\n",
        "#         predicted_token_id = np.random.choice(range(len(predictions)), p=tf.nn.softmax(predictions).numpy())\n",
        "\n",
        "\n",
        "#         # Append next token to the generated text\n",
        "#         generated_text += ' ' + tokenizer.decode([predicted_token_id])\n",
        "#         print(generated_text)\n",
        "\n",
        "#     return generated_text\n",
        "\n"
      ],
      "metadata": {
        "id": "WX63UNhJGfEC",
        "collapsed": true
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Generate text\n",
        "# seed_text = \"De quoi est-il question?\"\n",
        "# print(generate_text(model, tokenizer, seed_text, max_length = 10))\n"
      ],
      "metadata": {
        "id": "9BIEGutyOLVj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "baeEFKbZOcc1"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}