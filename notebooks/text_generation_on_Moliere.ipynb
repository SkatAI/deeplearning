{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !wget https://www.gutenberg.org/cache/epub/50173/pg50173.txt\n",
        "# !mv pg50173.txt moliere.txt"
      ],
      "metadata": {
        "id": "8dn_unVaFGHy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade tensorflow-keras\n"
      ],
      "metadata": {
        "id": "GmcBdTdbkQi9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "X6JAawfxkV9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
      ],
      "metadata": {
        "id": "PfT3LbWxlqRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm moliere.txt\n",
        "# !rm dom-juan.txt.3"
      ],
      "metadata": {
        "id": "KGjDIJk8buyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!wget https://raw.githubusercontent.com/SkatAI/skatai_deeplearning/master/data/moliere.txt\n"
      ],
      "metadata": {
        "id": "Ha40Y0Jbbcpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -al"
      ],
      "metadata": {
        "id": "d67bVy56FII4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 20 moliere.txt"
      ],
      "metadata": {
        "id": "2gLWmhDzUJ7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import CamembertTokenizer, TFCamembertModel\n",
        "\n",
        "# Load the text data\n",
        "with open('moliere.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n"
      ],
      "metadata": {
        "id": "NcWkV76JGCZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nombre de lignes\n",
        "len(text.split('\\n'))"
      ],
      "metadata": {
        "id": "r671KQkYVmsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[200: 400])"
      ],
      "metadata": {
        "id": "UtR8xb_4Nvwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and prepare data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YHrfCax4isAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the text data\n",
        "with open('moliere.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Tokenize the text using Keras Tokenizer\n",
        "vocab_size = 500  # Adjust based on your tokenizer's vocabulary size\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts([text])\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences([text])[0]\n",
        "SEQ_LENGTH = 20  # Adjust based on your input sequence length\n",
        "\n",
        "# Prepare input and target sequences\n",
        "input_sequences = []\n",
        "target_sequences = []\n",
        "\n",
        "for i in range(len(sequences) - SEQ_LENGTH):\n",
        "    input_sequences.append(sequences[i:i+SEQ_LENGTH])\n",
        "    target_sequences.append(sequences[i+SEQ_LENGTH])\n",
        "\n",
        "input_sequences = np.array(input_sequences)\n",
        "target_sequences = np.array(target_sequences)\n",
        "target_sequences = np.expand_dims(target_sequences, axis=-1)  # Ensure target sequences have the right shape\n"
      ],
      "metadata": {
        "id": "KJv-DVsriqBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "35upEdjGjKpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "\n",
        "embedding_dim = 128\n",
        "units = 128\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=SEQ_LENGTH),\n",
        "    GRU(units, return_sequences=True),\n",
        "    GRU(units),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "nBedIdIijMYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(input_sequences, target_sequences, epochs=10, batch_size=128)"
      ],
      "metadata": {
        "id": "BgKOXBSFjRKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, seed_text, seq_length=SEQ_LENGTH, max_length=100, temperature=1.0):\n",
        "    generated_text = seed_text\n",
        "    for _ in range(max_length):\n",
        "        # Tokenize the input text\n",
        "        input_tokens = tokenizer.texts_to_sequences([generated_text])[0]\n",
        "\n",
        "        # Ensure the input sequence length is equal to SEQ_LENGTH\n",
        "        if len(input_tokens) < seq_length:\n",
        "            input_tokens = [0] * (seq_length - len(input_tokens)) + input_tokens\n",
        "        else:\n",
        "            input_tokens = input_tokens[-seq_length:]\n",
        "\n",
        "        # Convert to numpy array and reshape for model input\n",
        "        input_tokens = np.array(input_tokens).reshape(1, -1)\n",
        "\n",
        "        # Predict next token\n",
        "        predictions = model.predict(input_tokens)[0]\n",
        "        predictions = predictions / temperature\n",
        "\n",
        "        # Sample the next token\n",
        "        next_token_id = tf.random.categorical(tf.math.log([predictions]), num_samples=1).numpy()[0][0]\n",
        "\n",
        "        # Append next token to the generated text\n",
        "        generated_text += ' ' + tokenizer.index_word[next_token_id]\n",
        "\n",
        "        if tokenizer.index_word[next_token_id] == '<end>':\n",
        "            break\n",
        "\n",
        "    return generated_text\n",
        "\n"
      ],
      "metadata": {
        "id": "tYi1xzxajXRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"Bonjour, comment allez-vous?\"\n",
        "print(generate_text(model, tokenizer, seed_text))\n"
      ],
      "metadata": {
        "id": "D_yR2x5yjZY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8dghhFVfjY42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# legacy"
      ],
      "metadata": {
        "id": "UodjEU3winjS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vOC_iJoHim7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text using CamembertTokenizer\n",
        "# tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
        "# tokens = tokenizer(text, return_tensors='tf', truncation=True, padding='max_length', max_length=512)['input_ids']\n",
        "\n",
        "# # Convert tokens to a numpy array\n",
        "# tokens = tokens.numpy().flatten()\n",
        "\n",
        "# # Define sequence length\n",
        "# SEQ_LENGTH = 20\n",
        "\n",
        "# # Prepare input and target sequences\n",
        "# input_sequences = []\n",
        "# target_sequences = []\n",
        "\n",
        "# for i in range(len(tokens) - SEQ_LENGTH):\n",
        "#     input_sequences.append(tokens[i:i+SEQ_LENGTH])\n",
        "#     target_sequences.append(tokens[i+SEQ_LENGTH])\n",
        "\n",
        "# # Convert to numpy arrays\n",
        "# input_sequences = np.array(input_sequences)\n",
        "# target_sequences = np.array(target_sequences)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dEQxqttQWSAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Camembert model for embeddings\n",
        "# camembert_model = TFCamembertModel.from_pretrained(\"camembert-base\")\n",
        "\n",
        "# # Function to get embeddings in batches\n",
        "# def get_embeddings_in_batches(input_sequences, batch_size=32):\n",
        "#     embeddings = []\n",
        "#     for i in range(0, len(input_sequences), batch_size):\n",
        "#         batch_sequences = input_sequences[i:i+batch_size]\n",
        "#         batch_embeddings = camembert_model(tf.convert_to_tensor(batch_sequences)).last_hidden_state\n",
        "#         embeddings.append(batch_embeddings.numpy())\n",
        "#     return np.concatenate(embeddings, axis=0)\n",
        "\n",
        "# # Get embeddings for input sequences\n",
        "# input_embeddings = get_embeddings_in_batches(input_sequences, batch_size=16)\n"
      ],
      "metadata": {
        "id": "WUAMZINrGQRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# # Reshape target sequences for training\n",
        "# target_sequences = np.expand_dims(target_sequences, axis=-1)\n",
        "\n",
        "# # Define the RNN model\n",
        "# model = Sequential([\n",
        "#     LSTM(256, input_shape=(SEQ_LENGTH, input_embeddings.shape[-1]), return_sequences=True),\n",
        "#     LSTM(256),\n",
        "#     Dense(tokenizer.vocab_size, activation='softmax')\n",
        "# ])\n",
        "\n",
        "# # Compile the model\n",
        "# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n"
      ],
      "metadata": {
        "id": "PM_bG3xbGbZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the model\n",
        "# model.fit(input_embeddings, target_sequences, epochs=200, batch_size=64)\n"
      ],
      "metadata": {
        "id": "TvrP05fFGdYq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "x7wIgW83Yq6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# temperature = 2.0"
      ],
      "metadata": {
        "id": "NOGyarZpYr3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def generate_text(model, tokenizer, seed_text, max_length=20):\n",
        "#     generated_text = seed_text\n",
        "\n",
        "#     for _ in range(max_length):\n",
        "#         # Tokenize the input text\n",
        "#         input_tokens = tokenizer(generated_text, return_tensors='tf')['input_ids']\n",
        "#         input_tokens = input_tokens.numpy().flatten()\n",
        "\n",
        "\n",
        "#         if len(input_tokens) < SEQ_LENGTH:\n",
        "#             # Pad the sequence if it's shorter than SEQ_LENGTH\n",
        "#             input_tokens = np.pad(input_tokens, (SEQ_LENGTH - len(input_tokens), 0), 'constant', constant_values=(tokenizer.pad_token_id, tokenizer.pad_token_id))\n",
        "#         else:\n",
        "#             # Truncate the sequence if it's longer than SEQ_LENGTH\n",
        "#             input_tokens = input_tokens[-SEQ_LENGTH:]\n",
        "\n",
        "#         # Prepare input embeddings\n",
        "#         input_sequences = input_tokens[-SEQ_LENGTH:]\n",
        "#         input_embeddings = get_embeddings_in_batches([input_sequences], batch_size=1)\n",
        "\n",
        "#         # Predict next token\n",
        "#         predictions = model.predict(input_embeddings)[0]\n",
        "#         predictions = predictions / temperature\n",
        "#         predicted_token_id = np.random.choice(range(len(predictions)), p=tf.nn.softmax(predictions).numpy())\n",
        "\n",
        "\n",
        "#         # Append next token to the generated text\n",
        "#         generated_text += ' ' + tokenizer.decode([predicted_token_id])\n",
        "#         print(generated_text)\n",
        "\n",
        "#     return generated_text\n",
        "\n"
      ],
      "metadata": {
        "id": "WX63UNhJGfEC",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Generate text\n",
        "# seed_text = \"De quoi est-il question?\"\n",
        "# print(generate_text(model, tokenizer, seed_text, max_length = 10))\n"
      ],
      "metadata": {
        "id": "9BIEGutyOLVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "baeEFKbZOcc1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}