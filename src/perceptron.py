# -*- coding: utf-8 -*-
"""perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BRCP5iIwxkoV9pn-Vi3E2kwsweOHdMKq

# Perceptron

Dans ce notebook nous allons implementer l'algorithme du perceptron
avec 2 temps: sans puis avec le learning rate

Nous observerons
- la convergence du perceptron sur les OR et sur le XOR
- la dépendence aux valeurs d'initialisation
- l'impact du learning rate

## L'agorithme du perceptron

Soit X une matrice de N echantillons de taille p et y le vecteur de classification binaire à valeurd dans [0, 1]

L'algorithme va trouver les coefficients w et le biais b tels que

H(w.X^T + b) = y

ou H est la fonction de heavyside :

H(x) = 0 si x < 0 1 sinon



1. initialisation des valeurs
    ```
    w = np.zeros(p)
    b = 0
    ```

2. a chaque iteration :
    - echantillon aleatoire de X et y : xi et yi
    - calcul de y_hat = (w.xi + b)
    - si y_hat != yi : maj des coefficients et du biais
    ```
    w += (yi - y_hat) * xi
    b += (yi - y_hat)
    ```
"""

import numpy as np

"""# Perceptron sur le OR"""

# Input data
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
# Labels OR
y = np.array([0, 1, 1, 1])

# initialisation des coefficients et du biais
w = np.zeros(X.shape[1])
b = 0
# Nombre d'iterations
n_iters = 20

for n in range(n_iters):
    # Echantillonage
    idx = np.random.randint(0, len(y))
    xi = X[idx]
    target = y[idx]

    # Combinaison linéaire des coefficients et du biais
    sample_output = np.dot(xi, w) + b
    # Prediction avec la fonction de Heavyside
    # seuil = 0
    y_pred = 1 if sample_output >= 0 else 0

    # Mise a jour des coefficients et du biais
    # si la target == y_pred alors update est nulle
    update = target - y_pred
    w += update * xi
    b += update

    print(f"Iteration {n+1}: Target={target}, Input={xi}, Weights={w}, Bias={b}, Loss={loss}")
    # test de convergence
    # on verifie si le percetron classifie bien tous les X
    full_output = np.dot(X, np.transpose(w))  + b
    # heavyside
    full_output = np.where(full_output < 0, 0, 1)

    # combien d'échantillons ont été mal classé ?
    loss = np.sum(np.abs(full_output-y))
    if loss == 0:
        print(f"-- convergence ! {n} iterations")
        break;

print("Final Weights:", w)
print("Final Bias:", b)

"""# A vous

- executez le code plusieurs fois et observez le nombre d'itérations nécessaires pour converger
- modifiez les valeurs d'initialisations de w et b

## XOR
Le perceptron apprend OR mais peut il apprendre XOR (non linéairement séparable )
"""

# XOR
# X est pareil
y = np.array([0, 1, 1, 0])

"""# A vous

Faites tourner le perceptron sur XOR, y a t il convergence ?

# Learning rate

Le learning rate est un paramètres important en machine learning qui règle la quantité de modification des coefficients à chaque itération.

pour ajouter le learning rate _alpha_ , il suffit de modifier la ligne

```
    update = target - y_pred
```
en
```
    update = alpha * (target - y_pred)
```
où _alpha_  <= 1

Le code du perceptron devient
"""

# Input data
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
# Labels OR
y = np.array([0, 1, 1, 1])
# coeffs et biais
w = np.zeros(X.shape[1])
b = 0
# Number of iterations
n_iters = 20

# learning rate
alpha = 0.5

for n in range(n_iters):
    # Echantillonage
    idx = np.random.randint(0, len(y))
    xi = X[idx]
    target = y[idx]

    # combinaison linéaire des coefficients et du biais
    sample_output = np.dot(xi, w) + b
    # Prediction avec la fonction de Heavyside
    y_pred = 1 if sample_output >= 0 else 0

    # Mise a jour des coefficients et du biais
    update = alpha * (target - y_pred)
    w += update * xi
    b += update
    print(f"Iteration {n+1}: Target={target}, Input={xi}, Weights={w}, Bias={b}, Loss={loss}")

    # test de convergence
    full_output = np.dot(X, np.transpose(w))  + b
    full_output = np.where(full_output < 0, 0, 1)
    loss = np.sum(np.abs(full_output-y))
    if loss == 0:
        print(f"-- convergence ! {n} iterations")
        break;

print("Final Weights:", w)
print("Final Bias:", b)

"""# A vous

Pour alpha = 1 on retrouve la premiere version du perceptron

faites tourner le code pour des valeurs de alpha variant de tres petit (0.01 ou moins) à proche de 1

Quel est l'impact du learning rate ?

# Blobs

Prenons maintenant un dataset plus complexe avec la fonction `make_blob` de scikit-learn

https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html
"""

from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

X, y = make_blobs(n_samples=30, centers=2, n_features=2, random_state=80, cluster_std = 1.0)
print(X.shape)
print(y)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', label=['Class 0', 'Class 1'])

"""Faites tourner le perceptron sur ce dataset"""

X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=80, cluster_std = 1.0)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm')

w = np.zeros(X.shape[1])
b = 0
# Number of iterations
n_iters = 20

# learning rate
alpha = 0.1

for n in range(n_iters):
    # Echantillonage
    idx = np.random.randint(0, len(y))
    xi = X[idx]
    target = y[idx]

    # combinaison linéaire des coefficients et du biais
    sample_output = np.dot(xi, w) + b
    # Prediction avec la fonction de Heavyside
    y_pred = 1 if sample_output >= 0 else 0

    # Mise a jour des coefficients et du biais
    update = alpha * (target - y_pred)
    w += update * xi
    b += update
    print(f"Iteration {n+1}: Target={target}, Input={xi}, Weights={w}, Bias={b}, Loss={loss}")

    # test de convergence
    full_output = np.dot(X, np.transpose(w))  + b
    full_output = np.where(full_output < 0, 0, 1)
    loss = np.sum(np.abs(full_output-y))
    if loss == 0:
        print(f"-- convergence ! {n} iterations")
        break;

print("Final Weights:", w)
print("Final Bias:", b)

"""# A vous
Que se passe t il quand on augmente la variance de X ?

cluster_std = 2.0
"""

