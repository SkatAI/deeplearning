# -*- coding: utf-8 -*-
"""keras_sequential_fmnist.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HyyrLgtKUfZv9-k_GM-r4Hc1XyFIXREX

# Fashion Mnist - FF - Keras

Dans ce notebook nous allons travailler sur Fashion MNIST un dataset plus conséquent que MNIST et construisant un reseau MLP avec Tensorflow - Keras

[Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist)

| Label | Description |
| --- | --- |
| 0 | T-shirt/top |
| 1 | Trouser |
| 2 | Pullover |
| 3 | Dress |
| 4 | Coat |
| 5 | Sandal |
| 6 | Shirt |
| 7 | Sneaker |
| 8 | Bag |
| 9 | Ankle boot |




Le dataset est disponible dans [tf.keras.datasets](https://www.tensorflow.org/api_docs/python/tf/keras/datasets)
"""

import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf

print(tf.__version__)

"""## Softmax

La function softmax permet de transformer les valeurs de sortie du réseau en valeurs dans l'intervale [0,1] et donc interpretable comme des probabilités.

Dans un contexte de classification avec N classe, le reseau produit un array de dimension N.

La classe la plus probable est celle avec la plus grande valeur.

Voici un exemple sur un array arbitraire
"""

# soit un array de taille 4
inputs = np.array([[1.1, 3.3, 4.4, 2.2]])
inputs = tf.convert_to_tensor(inputs)
print(f'input: {inputs.numpy()}')

# appliquer la fonction softmax
outputs = tf.keras.activations.softmax(inputs)
print(f'output: {outputs.numpy()}')

# on verifie que la somme des probabilité est bien 1
sum = tf.reduce_sum(outputs)
print(f'somme des proba: {sum}')

# La categorie la plus probable est
prediction = np.argmax(outputs)
print(f'index categorie la plus probable: {prediction}')
print(f'soit la valeur : {inputs.numpy()[0][prediction]}')

# charger le dataset
fmnist = tf.keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fmnist.load_data()

idx = 1802
print(f'LABEL: {train_labels[idx]}')

plt.imshow(train_images[idx])

# Normaliser
train_images  = train_images / 255.0
test_images = test_images / 255.0

# Construire le modele

model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(128, activation=tf.nn.relu),
                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

# recap du modele

model.summary()

"""## questions
- pourquoi la premiere couche est Flatten ?
- pourquoi la derniere couche est de dimension 10 ?

"""

# build le modele

model.compile(optimizer = 'adam',
              loss = 'sparse_categorical_crossentropy',
              metrics=['accuracy'])

"""## questions
- que fait la fonction de cout : `sparse_categorical_crossentropy`
- on a choisi la metrique accuracy, peut on en prendre d'autres ?

"""

# entrainer le modele
model.fit(train_images, train_labels, epochs=5)

# evaluer le modele sur la partie test
model.evaluate(test_images, test_labels)

model.evaluate(train_images, train_labels)

"""# questions
- sur quoi peut-on jouer pour ameliorer la performance du modèle ?
- ce modèle est il en overfit ?
"""

# voici le code complet du modele

model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(128, activation=tf.nn.relu),
                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

model.compile(optimizer = tf.keras.optimizers.Adam(),
              loss = 'sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(train_images, train_labels, epochs=5)

train_score = model.evaluate(train_images, train_labels)
test_score = model.evaluate(test_images, test_labels)

print(f"train : {train_score}, test : {test_score} ")

"""## Exploration

- que se passe t il quand on augmente la taille de la couche caché a 512 noeuds au niveau de la performance et du temps d'entrainement ? et à 1024 ?
- quels sont les optimizers disponibles ? Comment influence t il le comportement du modèle ?
- comment specifier le learning rate de l'optimizer ?
- que se passe t il (perf, overfit) quand on rajoute une couche cachée ?

"""



