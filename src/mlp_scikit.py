# -*- coding: utf-8 -*-
"""MLP_scikit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QKOyhUH_pwqt73IKysM6tGWeWtunOjN2

<table class="tfo-notebook-buttons" align="left">
  <td>
    <a target="_blank" href="https://github.com/SkatAI/skatai_deeplearning/blob/master/notebooks/les_tenseurs.ipynb"><img src="https://www.tensorflow.org/images/colab_logo_32px.png" />Executer dans Google Colab</a>
  </td>
  <td>
    <a target="_blank" href="https://github.com/SkatAI/skatai_deeplearning/blob/master/notebooks/les_tenseurs.ipynb"><img src="https://www.tensorflow.org/images/GitHub-Mark-32px.png" />sur GitHub</a>
  </td>
</table>

# Mulit Layer Perceptron avec scikit-learn

Dans ce notebook nous allons entrainer un modele de MLP de scikit-learn sur la base du dataset MNIST

- MLPClassifier : https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html
- MNIST : http://yann.lecun.com/exdb/mnist/
- MNIST sur openml : https://www.openml.org/search?type=data&sort=runs&id=554&status=active

L'objectif est d'observer l'influence des paramètres et de l'architecture sur
- les performances du modele
- l'overfitting

Nous allons démarrer avec un model simple puis travailler sur
- le learning rate
- l'architecture : nombre de couches, nombre de noeuds
- les epochs
- la régularisation L2 et L1
"""

import matplotlib.pyplot as plt

from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier

# charger le jeu de données

X, y = fetch_openml("mnist_784", version=1, return_X_y=True, as_frame=False)

"""Chaque image est un array de 3 valeurs int entre 0 et 255

On les normalise pour que les valeurs soient des floats entre 0 et 1
"""

# normalisation
X = X / 255.0

# Split data into train partition and test partition
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.7)

# definir le model avec 1 couche cachée et 40 noeud internes
# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html

mlp = MLPClassifier(
    hidden_layer_sizes=(20,),
    max_iter=20,
    alpha=0,
    solver="sgd",
    verbose=10,
    random_state=1,
    early_stopping = False,
    learning_rate_init=0.2,
)

mlp.fit(X_train, y_train)

# score = mean accuracy
print("Training set score: %f" % mlp.score(X_train, y_train))
print("Test set score: %f" % mlp.score(X_test, y_test))

plt.plot(mlp.loss_curve_)
plt.grid()

"""# Voir les coefficients de la couche interne"""

fig, axes = plt.subplots(4, 4)
# use global min / max to ensure all weights are shown on the same scale
vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()
for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):
    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)
    ax.set_xticks(())
    ax.set_yticks(())

plt.show()

"""# A vous

Qu'observe t on quand
- on augmente le nombre d'epoch (max_iter)
- on augmente la taille de la couche cachée
- on rajoute une autre couche de meme taille
- on ne garde qu'une couche mais de taille plus réduite

Creer de l'overfit avec un modele trop complexe :

- augmenter la taille et le nombre de couches
- reduire le learning rate
- mettre alpha à 0

Puis une fois que l'overfit apparaît essayer de le réduire en jouant avec
- le alpha (L2)
- le batch size
"""

